# Transformer Language Modeling on text8 Dataset (PyTorch) 
- Correct transformer implementation, set good base understanding for autoregressive language modeling
- Achieved perplexity of 6.47 on the text8 dataset (first 100M Wikipedia characters)
- Implemented a character-level autoregressive model using a TransformerEncoder with sliding-window sequence chunking and learned positional embeddings 
