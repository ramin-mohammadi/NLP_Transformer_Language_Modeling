{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- connect to T4 GPU\n",
        "- create data folder and drag-drop inside\n",
        "- create plots folder\n",
        "- drag drop rest of files"
      ],
      "metadata": {
        "id": "1OJ-ZMyIMvs2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAVHADO1kMuw",
        "outputId": "3aad5f59-b3c5-4b15-81cf-4f7b77027c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.16.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE Limitations of Part1 due to Decode method\n",
        "- decode method (tests our trained model) is hardcoded (and cannot modify as grader will likely have that implementation for decode)so assumes:\n",
        "  - using cpu\n",
        "  - no batching\n",
        "- therefore a more optimal approach would've been to:\n",
        "  - train and inference on gpu (faster training and inferencing) bc would allow use of larger transformer (larger d_model embedding dim and larger feedforward dim d_internal)\n",
        "    - on cpu, default transformer paper params: d_model=512, d_internal(dim_feedforward)=2048 ends up taking about 3 mins per epoch (not efficient)\n",
        "  - batching\n",
        "    - im able to batch for my training but note my inferencing (on dev (test) dataset) does not use batching (and decode method does not either) so output log_probs was handled differently for my loss function than for training bc in forward I had to remove batch dim if batchsize=1 -> since decode method argmax axis=1 assumes no batch dimension"
      ],
      "metadata": {
        "id": "iC3E3IrA6Xai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1\n",
        "- if --task BEFOREAFTER, in code comment positional encoding operation in Transformer forward\n",
        "- if --task BEFORE use positional encoding"
      ],
      "metadata": {
        "id": "lidV5-6uMUJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python letter_counting.py --task BEFOREAFTER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LznjSwtkrNc",
        "outputId": "c52107bf-6ae4-46cc-f8c3-25d57b71f1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(task='BEFOREAFTER', train='data/lettercounting-train.txt', dev='data/lettercounting-dev.txt', output_bundle_path='classifier-output.json')\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
            "10000 lines read in\n",
            "1000 lines read in\n",
            "Epoch 0 loss 225.520373\n",
            "Epoch 1 loss 67.804799\n",
            "Epoch 2 loss 17.145683\n",
            "Epoch 3 loss 6.210209\n",
            "Epoch 4 loss 3.075545\n",
            "Epoch 5 loss 1.740892\n",
            "Epoch 6 loss 6.657256\n",
            "Epoch 7 loss 0.877065\n",
            "Epoch 8 loss 0.701277\n",
            "Epoch 9 loss 0.563867\n",
            "Dev loss 2.114813\n",
            "INPUT 0: heir average albedo \n",
            "GOLD 0: array([0, 2, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2])\n",
            "PRED 0: array([0, 2, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2])\n",
            "INPUT 1: ed by rank and file \n",
            "GOLD 1: array([1, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
            "PRED 1: array([1, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
            "INPUT 2: s can also extend in\n",
            "GOLD 2: array([1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 2, 0, 2])\n",
            "PRED 2: array([1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 2, 0, 2])\n",
            "INPUT 3: erages between nine \n",
            "GOLD 3: array([2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2])\n",
            "PRED 3: array([2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2])\n",
            "INPUT 4:  that civilization n\n",
            "GOLD 4: array([2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 1])\n",
            "PRED 4: array([2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 1])\n",
            "Accuracy: 100 / 100 = 1.000000\n",
            "Training accuracy (100 exs):\n",
            "Accuracy: 2000 / 2000 = 1.000000\n",
            "Dev accuracy (whole set):\n",
            "Decoding on a large number of examples (1000); not printing or plotting\n",
            "Accuracy: 19993 / 20000 = 0.999650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python letter_counting.py --task BEFORE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cERVHbVPk8sV",
        "outputId": "07709227-69b2-49de-f7d6-f701a7119264"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(task='BEFORE', train='data/lettercounting-train.txt', dev='data/lettercounting-dev.txt', output_bundle_path='classifier-output.json')\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
            "10000 lines read in\n",
            "1000 lines read in\n",
            "Epoch 0 loss 215.220104\n",
            "Epoch 1 loss 117.297072\n",
            "Epoch 2 loss 44.137706\n",
            "Epoch 3 loss 19.995091\n",
            "Epoch 4 loss 12.224274\n",
            "Epoch 5 loss 8.147050\n",
            "Epoch 6 loss 5.890897\n",
            "Epoch 7 loss 4.034489\n",
            "Epoch 8 loss 3.350244\n",
            "Epoch 9 loss 2.986180\n",
            "Dev loss 14.087654\n",
            "INPUT 0: heir average albedo \n",
            "GOLD 0: array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2])\n",
            "PRED 0: array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2])\n",
            "INPUT 1: ed by rank and file \n",
            "GOLD 1: array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
            "PRED 1: array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 2])\n",
            "INPUT 2: s can also extend in\n",
            "GOLD 2: array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 2, 0, 2])\n",
            "PRED 2: array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 2, 0, 2])\n",
            "INPUT 3: erages between nine \n",
            "GOLD 3: array([0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 1, 1, 0, 2, 2, 2])\n",
            "PRED 3: array([0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 1, 1, 0, 2, 2, 2])\n",
            "INPUT 4:  that civilization n\n",
            "GOLD 4: array([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 2, 0, 0, 2, 1])\n",
            "PRED 4: array([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 2, 0, 0, 2, 1])\n",
            "Accuracy: 100 / 100 = 1.000000\n",
            "Training accuracy (100 exs):\n",
            "Accuracy: 1994 / 2000 = 0.997000\n",
            "Dev accuracy (whole set):\n",
            "Decoding on a large number of examples (1000); not printing or plotting\n",
            "Accuracy: 19920 / 20000 = 0.996000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "arr=[\n",
        "    np.array([1,2,3]),\n",
        "    np.array([4,5,6]),\n",
        "    np.array([7,8,9]),\n",
        "    np.array([11,4,1])\n",
        "]\n",
        "print(np.argmax(arr, axis=1))\n",
        "\n",
        "arr=[\n",
        "    np.array([1,2,3]),\n",
        "    np.array([4,5,6]),\n",
        "    np.array([7,8,9]),\n",
        "    np.array([11,4,1])\n",
        "]\n",
        "arr=[arr] # in decode method, it assumes no batch dimension, if there is one, the argmax is incorrect (wrong axis)\n",
        "print(np.argmax(arr, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_cNVBnCxTn9",
        "outputId": "9693bbe3-8c87-45e7-be2b-5249bcdf6327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 2 0]\n",
            "[[3 2 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "abxU1oJzs5L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python lm.py --model NEURAL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMQ5Pp8HsyeC",
        "outputId": "d6a84e20-4ef0-49d0-8ddc-e7c5dcdfe57e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(model='NEURAL', train_path='data/text8-100k.txt', dev_path='data/text8-dev.txt', output_bundle_path='output.json')\n",
            "100000 chars read in\n",
            "500 chars read in\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
            "First 100 characters of train:\n",
            " anarchism originated as a term of abuse first used against early working class radicals including t\n",
            "Starting epoch 0 - lr=1.000000e-04\n",
            "Epoch 0 training avg loss (nats/token): 2.715817, perplexity: 15.1170\n",
            "Epoch 0 dev   avg loss (nats/token): 2.392445, perplexity: 10.9402\n",
            "Starting epoch 1 - lr=1.000000e-04\n",
            "Epoch 1 training avg loss (nats/token): 2.431421, perplexity: 11.3750\n",
            "Epoch 1 dev   avg loss (nats/token): 2.323557, perplexity: 10.2119\n",
            "Starting epoch 2 - lr=1.000000e-04\n",
            "Epoch 2 training avg loss (nats/token): 2.368586, perplexity: 10.6823\n",
            "Epoch 2 dev   avg loss (nats/token): 2.263342, perplexity: 9.6152\n",
            "Starting epoch 3 - lr=1.000000e-04\n",
            "Epoch 3 training avg loss (nats/token): 2.298800, perplexity: 9.9622\n",
            "Epoch 3 dev   avg loss (nats/token): 2.189910, perplexity: 8.9344\n",
            "Starting epoch 4 - lr=1.000000e-04\n",
            "Epoch 4 training avg loss (nats/token): 2.224577, perplexity: 9.2496\n",
            "Epoch 4 dev   avg loss (nats/token): 2.115915, perplexity: 8.2972\n",
            "Starting epoch 5 - lr=1.000000e-04\n",
            "Epoch 5 training avg loss (nats/token): 2.161840, perplexity: 8.6871\n",
            "Epoch 5 dev   avg loss (nats/token): 2.056802, perplexity: 7.8209\n",
            "Starting epoch 6 - lr=1.000000e-04\n",
            "Epoch 6 training avg loss (nats/token): 2.108892, perplexity: 8.2391\n",
            "Epoch 6 dev   avg loss (nats/token): 2.006994, perplexity: 7.4409\n",
            "Starting epoch 7 - lr=1.000000e-04\n",
            "Epoch 7 training avg loss (nats/token): 2.064273, perplexity: 7.8796\n",
            "Epoch 7 dev   avg loss (nats/token): 1.959124, perplexity: 7.0931\n",
            "Starting epoch 8 - lr=1.000000e-04\n",
            "Epoch 8 training avg loss (nats/token): 2.026834, perplexity: 7.5900\n",
            "Epoch 8 dev   avg loss (nats/token): 1.923571, perplexity: 6.8454\n",
            "Starting epoch 9 - lr=1.000000e-04\n",
            "Epoch 9 training avg loss (nats/token): 1.995161, perplexity: 7.3534\n",
            "Epoch 9 dev   avg loss (nats/token): 1.888514, perplexity: 6.6095\n",
            "\n",
            "\n",
            "\n",
            "NORMALIZATION TEST\n",
            "VOCAB INDEXER: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
            "=====Results=====\n",
            "{\n",
            "  \"sane\": true,\n",
            "  \"normalizes\": true,\n",
            "  \"range\": true,\n",
            "  \"log_prob\": -934.4160018153489,\n",
            "  \"avg_log_prob\": -1.8688320036306978,\n",
            "  \"perplexity\": 6.480722516644136\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex = \"\"\n",
        "if ex == \"\":\n",
        "  print(\"true\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvf6SGYAHcYA",
        "outputId": "4a82fded-5c41-4281-8eee-f8888d4f1cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking and Creating dataset"
      ],
      "metadata": {
        "id": "NsnQo7tXJWH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import random\n",
        "def text_to_indices(text: str, vocab_index) -> List[int]:\n",
        "    \"\"\"Convert string to list of token indices using vocab_index.index_of(c).\"\"\"\n",
        "    return [vocab_index[c] for c in text]\n",
        "\n",
        "def chunk_non_overlapping(indices: List[int], seq_len: int,\n",
        "                          drop_last: bool = True,\n",
        "                          random_offset: bool = False) -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Partition indices into non-overlapping chunks of length seq_len.\n",
        "    If random_offset is True pick a start in [0, seq_len-1] each call (useful per-epoch).\n",
        "    If drop_last is False, the final short chunk is left-padded (requires pad_idx provided externally).\n",
        "    Returns list of chunks (each length <= seq_len). If you want guaranteed length seq_len,\n",
        "    either drop_last=True or post-pad/pad-left externally.\n",
        "    \"\"\"\n",
        "    if random_offset:\n",
        "        start = random.randint(0, seq_len - 1)\n",
        "    else:\n",
        "        start = 0\n",
        "    chunks = []\n",
        "    for i in range(start, len(indices), seq_len):\n",
        "        chunk = indices[i:i+seq_len]\n",
        "        if len(chunk) < seq_len and drop_last:\n",
        "            break\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "import numpy as np\n",
        "chunks = chunk_non_overlapping(np.arange(20), 3)\n",
        "print(chunks)\n",
        "chunks2 = chunk_non_overlapping(np.arange(20), 6, random_offset=True)\n",
        "print(chunks2)\n",
        "\n",
        "vocab_index = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n",
        "text = \"abcd\"\n",
        "print(text_to_indices(text, vocab_index))\n",
        "\n",
        "for _ in range(4):\n",
        "  random.shuffle(chunks)\n",
        "  print(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guLkKStTI8SE",
        "outputId": "bb8be66a-4285-4956-d0b2-04079d7d9f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8]), array([ 9, 10, 11]), array([12, 13, 14]), array([15, 16, 17])]\n",
            "[array([ 5,  6,  7,  8,  9, 10]), array([11, 12, 13, 14, 15, 16])]\n",
            "[0, 1, 2, 3]\n",
            "[array([3, 4, 5]), array([ 9, 10, 11]), array([0, 1, 2]), array([12, 13, 14]), array([6, 7, 8]), array([15, 16, 17])]\n",
            "[array([6, 7, 8]), array([3, 4, 5]), array([15, 16, 17]), array([12, 13, 14]), array([0, 1, 2]), array([ 9, 10, 11])]\n",
            "[array([ 9, 10, 11]), array([15, 16, 17]), array([12, 13, 14]), array([6, 7, 8]), array([3, 4, 5]), array([0, 1, 2])]\n",
            "[array([15, 16, 17]), array([0, 1, 2]), array([ 9, 10, 11]), array([12, 13, 14]), array([6, 7, 8]), array([3, 4, 5])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batching\n",
        "import torch\n",
        "batch_size = 4\n",
        "i=0\n",
        "print(chunks)\n",
        "for b in range(0, len(chunks), batch_size):\n",
        "  batch_chunks = chunks[b:b+batch_size]\n",
        "  batch_tensor = torch.tensor(np.array(batch_chunks), dtype=torch.long) # (batch_size, seq_len)\n",
        "  print(f\"Batch {i}\")\n",
        "  print(batch_tensor)\n",
        "  i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksvRA6kJOPJO",
        "outputId": "8494f429-15a4-4cb0-e5fa-1091558af20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([15, 16, 17]), array([0, 1, 2]), array([ 9, 10, 11]), array([12, 13, 14]), array([6, 7, 8]), array([3, 4, 5])]\n",
            "Batch 0\n",
            "tensor([[15, 16, 17],\n",
            "        [ 0,  1,  2],\n",
            "        [ 9, 10, 11],\n",
            "        [12, 13, 14]])\n",
            "Batch 1\n",
            "tensor([[6, 7, 8],\n",
            "        [3, 4, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output of model with loss\n",
        "arr = np.array([\n",
        "    [[0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9]],\n",
        "\n",
        "    [[1.1, 1.2, 1.3],\n",
        "     [1.4, 1.5, 1.6],\n",
        "     [1.7, 1.8, 1.9]]\n",
        "    ])\n",
        "logits = torch.tensor(arr, dtype=torch.float)\n",
        "print(logits.shape) # (2,3,3) -> (batch_size, seq_len, vocab_size)\n",
        "print(logits)\n",
        "# logits = logits.reshape(-1, logits.shape[-1])\n",
        "# targets = batch_tensor.reshape(-1)\n",
        "logits = logits.view(-1, logits.shape[-1])\n",
        "targets = batch_tensor.view(-1)\n",
        "print(\"\\nReshape For loss\")\n",
        "print(logits.shape)\n",
        "print(targets.shape)\n",
        "print(logits)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyRhimgfRFLF",
        "outputId": "0cbcf9b9-9ca6-4646-bd67-c8618ee0c34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 3])\n",
            "tensor([[[0.1000, 0.2000, 0.3000],\n",
            "         [0.4000, 0.5000, 0.6000],\n",
            "         [0.7000, 0.8000, 0.9000]],\n",
            "\n",
            "        [[1.1000, 1.2000, 1.3000],\n",
            "         [1.4000, 1.5000, 1.6000],\n",
            "         [1.7000, 1.8000, 1.9000]]])\n",
            "\n",
            "Reshape For loss\n",
            "torch.Size([6, 3])\n",
            "torch.Size([6])\n",
            "tensor([[0.1000, 0.2000, 0.3000],\n",
            "        [0.4000, 0.5000, 0.6000],\n",
            "        [0.7000, 0.8000, 0.9000],\n",
            "        [1.1000, 1.2000, 1.3000],\n",
            "        [1.4000, 1.5000, 1.6000],\n",
            "        [1.7000, 1.8000, 1.9000]])\n",
            "tensor([6, 7, 8, 3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Forward"
      ],
      "metadata": {
        "id": "h-3C6lfwYQJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = torch.tensor([1,2,3,4])\n",
        "if len(indices.shape) == 1:\n",
        "  indices = indices.unsqueeze(0)\n",
        "print(indices)\n",
        "print(indices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58HPMHzTYSAo",
        "outputId": "76a7b6fd-0ac4-41bd-d38b-df0f30cc1509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3, 4]])\n",
            "torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = torch.tensor([1,2,3,4]).unsqueeze(0)\n",
        "seq_len = 2\n",
        "if indices.shape[1] > seq_len:\n",
        "    indices = indices[:, -seq_len:] # (batch_size, seq_len)\n",
        "print(indices)\n",
        "print(indices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsPtCY3YTLJF",
        "outputId": "af292d23-7877-4fdf-d250-40eaf7ff1a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 4]])\n",
            "torch.Size([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = torch.tensor([3,4]).unsqueeze(0)\n",
        "seq_len = 6\n",
        "emb = torch.nn.Embedding(5, 3, padding_idx=0)\n",
        "# RIGHT ALIGNED PADDING\n",
        "if indices.shape[1] < seq_len:\n",
        "    # pad with PAD token index at the beginning if sequence is shorter than seq_len\n",
        "    pad_length = seq_len - indices.shape[1]\n",
        "    pad_idx = emb.padding_idx\n",
        "    pad_tensor = torch.full((indices.shape[0], pad_length), pad_idx, dtype=torch.long, device=indices.device) # (batch_size, pad_length)\n",
        "    indices = torch.cat([pad_tensor, indices], dim=1)\n",
        "print(indices)\n",
        "print(indices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kzO38X5ZYbq",
        "outputId": "470772b0-6c8c-42bc-c36d-f915836f008a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 3, 4]])\n",
            "torch.Size([1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(indices, \"\\nAfter roll:\")\n",
        "indices = torch.roll(indices, shifts=1, dims=1)\n",
        "print(indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggNqoyFuZiDq",
        "outputId": "48b52e97-c157-48c0-94fd-56f6da0aa91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 3, 4]]) \n",
            "After roll:\n",
            "tensor([[4, 0, 0, 0, 0, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "space_idx = 2\n",
        "indices[:, 0] = space_idx\n",
        "print(indices)\n",
        "x=emb(indices)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "252yL555dYuA",
        "outputId": "f3eee6fa-cbd3-4d98-d8b7-f779ce1ede99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 0, 0, 0, 0, 3]])\n",
            "tensor([[[ 0.6382, -0.6015,  0.7876],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.6401, -0.1063,  0.2540]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batched=True\n",
        "input_size = x.shape[-2]\n",
        "pos_emb = torch.nn.Embedding(input_size, 3)\n",
        "indices_to_embed = torch.tensor(np.asarray(range(0, input_size))).type(torch.LongTensor)\n",
        "if batched:\n",
        "    # Use unsqueeze to form a [1, seq len, embedding dim] tensor -- broadcasting will ensure that this\n",
        "    # gets added correctly across the batch\n",
        "    print(indices_to_embed)\n",
        "    emb_unsq = pos_emb(indices_to_embed).unsqueeze(0)\n",
        "print(emb_unsq)\n",
        "print(\"\\nSum\\n\", x+emb_unsq)\n",
        "\n",
        "x_one_more_sample = torch.cat((x, emb_unsq), dim=0)\n",
        "print(x_one_more_sample.shape)\n",
        "print(x_one_more_sample)\n",
        "\n",
        "print(\"\\nSum\\n:\", x_one_more_sample+emb_unsq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GT0YhsofPOm",
        "outputId": "06ff9169-4884-4fee-8541-bdaf6b7041dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5])\n",
            "tensor([[[ 0.4769,  1.3433,  0.3484],\n",
            "         [-2.1324,  1.0726, -0.6919],\n",
            "         [-0.3836, -0.6994,  1.3595],\n",
            "         [ 2.0871,  0.1587,  0.5033],\n",
            "         [-0.0736, -0.6491, -0.2882],\n",
            "         [-0.3321, -0.0394,  0.5740]]], grad_fn=<UnsqueezeBackward0>)\n",
            "\n",
            "Sum\n",
            " tensor([[[ 1.1151,  0.7418,  1.1360],\n",
            "         [-2.1324,  1.0726, -0.6919],\n",
            "         [-0.3836, -0.6994,  1.3595],\n",
            "         [ 2.0871,  0.1587,  0.5033],\n",
            "         [-0.0736, -0.6491, -0.2882],\n",
            "         [ 0.3079, -0.1457,  0.8281]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([2, 6, 3])\n",
            "tensor([[[ 0.6382, -0.6015,  0.7876],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.6401, -0.1063,  0.2540]],\n",
            "\n",
            "        [[ 0.4769,  1.3433,  0.3484],\n",
            "         [-2.1324,  1.0726, -0.6919],\n",
            "         [-0.3836, -0.6994,  1.3595],\n",
            "         [ 2.0871,  0.1587,  0.5033],\n",
            "         [-0.0736, -0.6491, -0.2882],\n",
            "         [-0.3321, -0.0394,  0.5740]]], grad_fn=<CatBackward0>)\n",
            "\n",
            "Sum\n",
            ": tensor([[[ 1.1151,  0.7418,  1.1360],\n",
            "         [-2.1324,  1.0726, -0.6919],\n",
            "         [-0.3836, -0.6994,  1.3595],\n",
            "         [ 2.0871,  0.1587,  0.5033],\n",
            "         [-0.0736, -0.6491, -0.2882],\n",
            "         [ 0.3079, -0.1457,  0.8281]],\n",
            "\n",
            "        [[ 0.9539,  2.6866,  0.6968],\n",
            "         [-4.2647,  2.1452, -1.3837],\n",
            "         [-0.7672, -1.3988,  2.7190],\n",
            "         [ 4.1742,  0.3174,  1.0066],\n",
            "         [-0.1472, -1.2981, -0.5764],\n",
            "         [-0.6643, -0.0788,  1.1481]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_emb = torch.nn.Embedding(5, 3)\n",
        "with torch.no_grad():\n",
        "  test_emb.weight[3].zero_()\n",
        "print(test_emb.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rpIVwnIh8iT",
        "outputId": "329aa24e-5723-4ae5-a692-b315128c4020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.8021,  0.8342,  0.4761],\n",
            "        [ 0.4370,  0.2659, -0.9019],\n",
            "        [ 0.3983, -0.9058,  0.0152],\n",
            "        [ 0.0000,  0.0000,  0.0000],\n",
            "        [-0.4759,  0.0561,  0.5250]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get_next_char_log_probs and get_log_prob_sequence tests"
      ],
      "metadata": {
        "id": "whs9T1qJizoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "def index_of(char):\n",
        "  vocab = {chr(i): i - ord('a') for i in range(ord('a'), ord('z') + 1)}\n",
        "  vocab[' '] = 26\n",
        "  vocab['#'] = 27\n",
        "  return vocab[char]\n",
        "\n",
        "def get_next_char_log_probs(context):\n",
        "  # make sure here to be in eval mode and detach from gpu (place back onto cpu) and convert to numpy array\n",
        "  context_indices = [index_of(c) for c in context]\n",
        "  context_tensor = torch.tensor(context_indices, dtype=torch.long) # (seq_len,)\n",
        "  print(\"getnextchar\\nContext: \", context)\n",
        "  print(\"Context tensor: \", context_tensor, \"\\n\")\n",
        "\n",
        "  logits = torch.arange(5*28, dtype=torch.float).view(1, 5, 28) # (1, seq_len, vocab_size)\n",
        "  print(\"Logits shape: \", logits.shape)\n",
        "  print(\"Logits: \", logits)\n",
        "  last_logits = logits[0, -1, :] # (vocab_size,)\n",
        "  print(\"Last logits: \", last_logits)\n",
        "  log_probs = torch.log_softmax(last_logits, dim=0) # (vocab_size,)\n",
        "  print(\"Log probs: \", log_probs)\n",
        "  return log_probs.numpy()\n",
        "\n",
        "def get_log_prob_sequence(next_chars, context):\n",
        "  total_log_prob = 0.0\n",
        "  cur_context = context\n",
        "  # if next_chars is a string,\n",
        "  for c in next_chars:\n",
        "      print(\"Current c: \", c)\n",
        "      print(\"Current context: \", cur_context)\n",
        "      logp = get_next_char_log_probs(cur_context)  # numpy array of log-probs\n",
        "      char_idx = index_of(c)\n",
        "      print(\"Char idx: \", char_idx)\n",
        "      print(\"logp[char_idx]: \", logp[char_idx])\n",
        "      total_log_prob += float(logp[char_idx])\n",
        "      cur_context = cur_context + c\n",
        "  return total_log_prob\n",
        "\n",
        "\n",
        "contexts = [\" \", \" a person \", \" some person \"]\n",
        "next_seqs = [\"s\", \"sits\", \"stands\"]\n",
        "sane = True\n",
        "for context in contexts:\n",
        "    for next_seq in next_seqs:\n",
        "        print(\"\\n\\nContext: \", context)\n",
        "        print(\"Next seq: \", next_seq)\n",
        "        log_prob = get_log_prob_sequence(next_seq, context)\n",
        "        print(\"Log prob SUM: \", log_prob)\n",
        "        # if log_prob > 0.0:\n",
        "        #     sane = False\n",
        "        #     print(\"ERROR: sanity checks failed, LM log probability %f is invalid\" % (log_prob))\n",
        "        # log_prob_from_single_probs = 0.0\n",
        "        # for i in range(0, len(next_seq)):\n",
        "        #     # print(repr(context + next_seq[0:i]))\n",
        "        #     # print(repr(next_seq[i]))\n",
        "        #     next_char_log_probs = lm.get_next_char_log_probs(context + next_seq[0:i]) # gets prob of next char given context + previous chars\n",
        "        #     # print(repr(next_char_log_probs))\n",
        "        #     log_prob_from_single_probs += next_char_log_probs[vocab_index.index_of(next_seq[i])] # acquire prob of ground truth char and sum\n",
        "        # if abs(log_prob_from_single_probs - log_prob) > 1e-3:\n",
        "        #     # if difference between result of get_log_prob_sequence and summing get_next_char_log_probs is too large,\n",
        "        #     # then implementation of either or of the methods are wrong (results may have some variance but should be < 1e-3 difference)\n",
        "        #     sane = False\n",
        "        #     print(\"ERROR: sanity checks failed, LM prob from sequence and single characters disagree: %f %f\" % (log_prob, log_prob_from_single_probs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXa6MNhsi76a",
        "outputId": "3e968d5e-b751-4f1c-eca4-8947bdf28e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Context:   \n",
            "Next seq:  s\n",
            "Current c:  s\n",
            "Current context:   \n",
            "getnextchar\n",
            "Context:   \n",
            "Context tensor:  tensor([26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -9.458675384521484\n",
            "\n",
            "\n",
            "Context:   \n",
            "Next seq:  sits\n",
            "Current c:  s\n",
            "Current context:   \n",
            "getnextchar\n",
            "Context:   \n",
            "Context tensor:  tensor([26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Current c:  i\n",
            "Current context:   s\n",
            "getnextchar\n",
            "Context:   s\n",
            "Context tensor:  tensor([26, 18]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  8\n",
            "logp[char_idx]:  -19.458675\n",
            "Current c:  t\n",
            "Current context:   si\n",
            "getnextchar\n",
            "Context:   si\n",
            "Context tensor:  tensor([26, 18,  8]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  19\n",
            "logp[char_idx]:  -8.458675\n",
            "Current c:  s\n",
            "Current context:   sit\n",
            "getnextchar\n",
            "Context:   sit\n",
            "Context tensor:  tensor([26, 18,  8, 19]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -46.83470153808594\n",
            "\n",
            "\n",
            "Context:   \n",
            "Next seq:  stands\n",
            "Current c:  s\n",
            "Current context:   \n",
            "getnextchar\n",
            "Context:   \n",
            "Context tensor:  tensor([26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Current c:  t\n",
            "Current context:   s\n",
            "getnextchar\n",
            "Context:   s\n",
            "Context tensor:  tensor([26, 18]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  19\n",
            "logp[char_idx]:  -8.458675\n",
            "Current c:  a\n",
            "Current context:   st\n",
            "getnextchar\n",
            "Context:   st\n",
            "Context tensor:  tensor([26, 18, 19]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  0\n",
            "logp[char_idx]:  -27.458675\n",
            "Current c:  n\n",
            "Current context:   sta\n",
            "getnextchar\n",
            "Context:   sta\n",
            "Context tensor:  tensor([26, 18, 19,  0]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  13\n",
            "logp[char_idx]:  -14.458675\n",
            "Current c:  d\n",
            "Current context:   stan\n",
            "getnextchar\n",
            "Context:   stan\n",
            "Context tensor:  tensor([26, 18, 19,  0, 13]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  3\n",
            "logp[char_idx]:  -24.458675\n",
            "Current c:  s\n",
            "Current context:   stand\n",
            "getnextchar\n",
            "Context:   stand\n",
            "Context tensor:  tensor([26, 18, 19,  0, 13,  3]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -93.7520523071289\n",
            "\n",
            "\n",
            "Context:   a person \n",
            "Next seq:  s\n",
            "Current c:  s\n",
            "Current context:   a person \n",
            "getnextchar\n",
            "Context:   a person \n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -9.458675384521484\n",
            "\n",
            "\n",
            "Context:   a person \n",
            "Next seq:  sits\n",
            "Current c:  s\n",
            "Current context:   a person \n",
            "getnextchar\n",
            "Context:   a person \n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Current c:  i\n",
            "Current context:   a person s\n",
            "getnextchar\n",
            "Context:   a person s\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  8\n",
            "logp[char_idx]:  -19.458675\n",
            "Current c:  t\n",
            "Current context:   a person si\n",
            "getnextchar\n",
            "Context:   a person si\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18,  8]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  19\n",
            "logp[char_idx]:  -8.458675\n",
            "Current c:  s\n",
            "Current context:   a person sit\n",
            "getnextchar\n",
            "Context:   a person sit\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18,  8, 19]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -46.83470153808594\n",
            "\n",
            "\n",
            "Context:   a person \n",
            "Next seq:  stands\n",
            "Current c:  s\n",
            "Current context:   a person \n",
            "getnextchar\n",
            "Context:   a person \n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Current c:  t\n",
            "Current context:   a person s\n",
            "getnextchar\n",
            "Context:   a person s\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  19\n",
            "logp[char_idx]:  -8.458675\n",
            "Current c:  a\n",
            "Current context:   a person st\n",
            "getnextchar\n",
            "Context:   a person st\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  0\n",
            "logp[char_idx]:  -27.458675\n",
            "Current c:  n\n",
            "Current context:   a person sta\n",
            "getnextchar\n",
            "Context:   a person sta\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19,  0]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  13\n",
            "logp[char_idx]:  -14.458675\n",
            "Current c:  d\n",
            "Current context:   a person stan\n",
            "getnextchar\n",
            "Context:   a person stan\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19,  0, 13]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  3\n",
            "logp[char_idx]:  -24.458675\n",
            "Current c:  s\n",
            "Current context:   a person stand\n",
            "getnextchar\n",
            "Context:   a person stand\n",
            "Context tensor:  tensor([26,  0, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19,  0, 13,  3]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -93.7520523071289\n",
            "\n",
            "\n",
            "Context:   some person \n",
            "Next seq:  s\n",
            "Current c:  s\n",
            "Current context:   some person \n",
            "getnextchar\n",
            "Context:   some person \n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -9.458675384521484\n",
            "\n",
            "\n",
            "Context:   some person \n",
            "Next seq:  sits\n",
            "Current c:  s\n",
            "Current context:   some person \n",
            "getnextchar\n",
            "Context:   some person \n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Current c:  i\n",
            "Current context:   some person s\n",
            "getnextchar\n",
            "Context:   some person s\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  8\n",
            "logp[char_idx]:  -19.458675\n",
            "Current c:  t\n",
            "Current context:   some person si\n",
            "getnextchar\n",
            "Context:   some person si\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18,  8]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  19\n",
            "logp[char_idx]:  -8.458675\n",
            "Current c:  s\n",
            "Current context:   some person sit\n",
            "getnextchar\n",
            "Context:   some person sit\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18,  8, 19]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -46.83470153808594\n",
            "\n",
            "\n",
            "Context:   some person \n",
            "Next seq:  stands\n",
            "Current c:  s\n",
            "Current context:   some person \n",
            "getnextchar\n",
            "Context:   some person \n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Current c:  t\n",
            "Current context:   some person s\n",
            "getnextchar\n",
            "Context:   some person s\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  19\n",
            "logp[char_idx]:  -8.458675\n",
            "Current c:  a\n",
            "Current context:   some person st\n",
            "getnextchar\n",
            "Context:   some person st\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  0\n",
            "logp[char_idx]:  -27.458675\n",
            "Current c:  n\n",
            "Current context:   some person sta\n",
            "getnextchar\n",
            "Context:   some person sta\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19,  0]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  13\n",
            "logp[char_idx]:  -14.458675\n",
            "Current c:  d\n",
            "Current context:   some person stan\n",
            "getnextchar\n",
            "Context:   some person stan\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19,  0, 13]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  3\n",
            "logp[char_idx]:  -24.458675\n",
            "Current c:  s\n",
            "Current context:   some person stand\n",
            "getnextchar\n",
            "Context:   some person stand\n",
            "Context tensor:  tensor([26, 18, 14, 12,  4, 26, 15,  4, 17, 18, 14, 13, 26, 18, 19,  0, 13,  3]) \n",
            "\n",
            "Logits shape:  torch.Size([1, 5, 28])\n",
            "Logits:  tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
            "           11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
            "           22.,  23.,  24.,  25.,  26.,  27.],\n",
            "         [ 28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
            "           39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
            "           50.,  51.,  52.,  53.,  54.,  55.],\n",
            "         [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
            "           67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
            "           78.,  79.,  80.,  81.,  82.,  83.],\n",
            "         [ 84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
            "           95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105.,\n",
            "          106., 107., 108., 109., 110., 111.],\n",
            "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
            "          123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,\n",
            "          134., 135., 136., 137., 138., 139.]]])\n",
            "Last logits:  tensor([112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123.,\n",
            "        124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135.,\n",
            "        136., 137., 138., 139.])\n",
            "Log probs:  tensor([-27.4587, -26.4587, -25.4587, -24.4587, -23.4587, -22.4587, -21.4587,\n",
            "        -20.4587, -19.4587, -18.4587, -17.4587, -16.4587, -15.4587, -14.4587,\n",
            "        -13.4587, -12.4587, -11.4587, -10.4587,  -9.4587,  -8.4587,  -7.4587,\n",
            "         -6.4587,  -5.4587,  -4.4587,  -3.4587,  -2.4587,  -1.4587,  -0.4587])\n",
            "Char idx:  18\n",
            "logp[char_idx]:  -9.458675\n",
            "Log prob SUM:  -93.7520523071289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32d3a3c3",
        "outputId": "3ddafc81-1919-4bb8-e182-f9b8cf05207b"
      },
      "source": [
        "vocab = {chr(i): i - ord('a') for i in range(ord('a'), ord('z') + 1)}\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sliding chunks"
      ],
      "metadata": {
        "id": "-Yj3bJgIAEV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def sliding_window_chunks(indices: List[int], seq_len: int, stride: int = 1, drop_short: bool = True) -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Create overlapping chunks using a sliding window.\n",
        "    - stride < seq_len will create overlapping windows.\n",
        "    - drop_short: if True drop the final partial window, otherwise include it as a shorter chunk.\n",
        "    Returns a list of chunks (each of length seq_len, except possibly the final one when drop_short=False).\n",
        "    \"\"\"\n",
        "    if len(indices) < seq_len:\n",
        "        return [] if drop_short else [indices]\n",
        "    chunks = [indices[i:i+seq_len] for i in range(0, len(indices) - seq_len + 1, stride)]\n",
        "    if not drop_short:\n",
        "        # include any trailing short tail\n",
        "        tail_start = (len(indices) - seq_len + 1) + ((len(indices) - seq_len) % stride)\n",
        "        tail = indices[tail_start + seq_len:]\n",
        "        if tail:\n",
        "            chunks.append(tail)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "seq_len = 6\n",
        "stride = max(1, seq_len // 2)\n",
        "train_chunks = sliding_window_chunks(np.arange(20), seq_len=seq_len, stride=stride, drop_short=True)\n",
        "print(np.shape(train_chunks))\n",
        "print(train_chunks)\n",
        "\n",
        "seq_len = 6\n",
        "stride = seq_len\n",
        "train_chunks = sliding_window_chunks(np.arange(20), seq_len=seq_len, stride=stride, drop_short=True)\n",
        "print(np.shape(train_chunks))\n",
        "print(train_chunks)\n",
        "\n",
        "seq_len = 6\n",
        "stride = 1\n",
        "train_chunks = sliding_window_chunks(np.arange(20), seq_len=seq_len, stride=stride, drop_short=True)\n",
        "print(np.shape(train_chunks))\n",
        "print(train_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMnl0VMNADgX",
        "outputId": "8b718875-ce28-4df4-d3c1-baa0dfd673ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 6)\n",
            "[array([0, 1, 2, 3, 4, 5]), array([3, 4, 5, 6, 7, 8]), array([ 6,  7,  8,  9, 10, 11]), array([ 9, 10, 11, 12, 13, 14]), array([12, 13, 14, 15, 16, 17])]\n",
            "(3, 6)\n",
            "[array([0, 1, 2, 3, 4, 5]), array([ 6,  7,  8,  9, 10, 11]), array([12, 13, 14, 15, 16, 17])]\n",
            "(15, 6)\n",
            "[array([0, 1, 2, 3, 4, 5]), array([1, 2, 3, 4, 5, 6]), array([2, 3, 4, 5, 6, 7]), array([3, 4, 5, 6, 7, 8]), array([4, 5, 6, 7, 8, 9]), array([ 5,  6,  7,  8,  9, 10]), array([ 6,  7,  8,  9, 10, 11]), array([ 7,  8,  9, 10, 11, 12]), array([ 8,  9, 10, 11, 12, 13]), array([ 9, 10, 11, 12, 13, 14]), array([10, 11, 12, 13, 14, 15]), array([11, 12, 13, 14, 15, 16]), array([12, 13, 14, 15, 16, 17]), array([13, 14, 15, 16, 17, 18]), array([14, 15, 16, 17, 18, 19])]\n"
          ]
        }
      ]
    }
  ]
}